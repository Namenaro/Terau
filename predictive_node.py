# -*- coding: utf-8 -*
import os
import matplotlib.pyplot as plt

import utils
import data_generator
import ensemble
import dropout_regressor
import visualisator
import numpy.exp as exp

# Модель должна инкапсулировать какую-то закономерность в данных.
# Разные модели должны инкапсулировать разные закономерности (каждая эксперт в чем-то своем).
# На уровне ансамбля мы считаем, что закономерности условны и существуют во времени.
# Т.е. объект "закономерность типа А" :
#  - возникает при определенных окружающих условиях,
#  - существует в течение какого-то времени и
#  - исчезает.
# В каждый момент времени активны не все предсказатели: некоторые закономерности взаимоисключающи (не могут наблюдаться одновременно)
# Если предсказатель сам не знает границы своей компетентности, то коррректировку вносит Смотрящий.
# Какой из предсказателей ансамбля генерирует предсказание в данный момент решается компромиссом предсказателей и смотрящего.
class PredictiveNode:
    def __init__(self, id, regressor):
        # id ноды в ансамбле, названается на уровне ансамбля, нужна для структуры данных типа граф
        self.id = id

        # модель, которая на вход получает точку (точки) данных и делает для них регрессию
        # а так же способна обучаться на ошибках с регулируемой амплитудой обучения
        # и сожержит в себе стохастичность (обеспечивая тем самым понятие "уверенность модели")
        self.model = regressor

        # нужно детектить ситуацию, когда:
        #  -  несколько моделек пытаются инкапсулировать одну и ту же закономерность и
        #  -  имеют на то примерно равные шансы
        # Это бывает когда на многих точках они дают похожие пары "ошибка, уверенность".
        #  с каждой такой похожестью мы увеличиваем вес синоним-связи между этими нодами
        # когда становится понтяно, что две ноды синонимы, можно смело удалять одну.
        self.synonim_links = {} # id : weight

        # насколько была полезна эта нода
        # пропорционально:
        #  - (1)  кол-ву верных предсказаний при высокой уверенности
        #  - (2) высокой неувереннсти при неверных предсказаниях
        # Итоговое значение является компромиссом (1) и (2):
        # - если модель везде выдается высокую неуверенность и неверные предсказания,
        #                   то она бесполезна - она не смогла словить закономерность.
        # - если модель часто (на точках Х) делает правильные уверенные предскаазния,
        #                   но временами выдает неверное уверенно (на точках У),
        #                   то последнее ей можно простить: ведь она поймала закономерность!
        #                   В этомс случае корректировку итогового предсказания надо возложить
        #                   на "смотрящего".
        self.how_useful = 0

        # на скольких точках признана победителем в ансамбле
        self.was_winner = 0

        # сколько тактов возраст ноды
        self.age = 0



    def f_useful_in_point(self, err, uns):

        # err, unsertainty >= 0

        # при прочих равных, чем меньше ошибка, тем полезней модель
        # при маленькой ошибке, чем больше уевренность тем лучше
        # при большой ошибке, чем меньше уверенность, тем лучше


        #upd = 0.05* y/e^(-x/7) + 2/e^(x^3 + y) http://web.monroecc.edu/manila/webfiles/pseeburger/CalcPlot3D/
        # первое слагаемое - награда за большую неуверенность при большой ошибке и наказаниае за большую уверенность при большой ошибке
        # второе слагаемое - дает пик награды за маленькую неуверенность при маленькой ошибке
        reward = 0.05* uns/exp(-err/7) + 2/exp(err^3 + uns)
        return reward




